{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "d73afe0d",
      "metadata": {
        "id": "d73afe0d"
      },
      "source": [
        "This notebook is heavily inspired from [Harvard transformer implementation](https://nlp.seas.harvard.edu/annotated-transformer/) and [Umar Jamil's transformer implementation](https://youtu.be/ISNdQcPhsts?si=_1mO7CBcvFHg15cJ)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b334792e",
      "metadata": {
        "id": "b334792e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import math"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "487a76b7",
      "metadata": {
        "id": "487a76b7"
      },
      "source": [
        "### Input Embedding\n",
        "\n",
        "<!-- TODO:\n",
        "\n",
        "- explore what `nn.Embedding` does\n",
        " -->"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7932669e",
      "metadata": {
        "id": "7932669e"
      },
      "outputs": [],
      "source": [
        "class InputEmbedding(nn.Module):\n",
        "    def __init__(self, d_model: int, vocab_size: int) -> None:\n",
        "        super(InputEmbedding, self).__init__()\n",
        "        # you can also do this:\n",
        "        # super().__init__()\n",
        "        self.d_model = d_model  # in this paper, it 512\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.embedding(x) * math.sqrt(self.d_model)\n",
        "        # check the last line on page 5:\n",
        "        # \"In the embedding layers, we multiply those weights by d model.\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5db4d201",
      "metadata": {
        "id": "5db4d201"
      },
      "source": [
        "### Positional Encoding\n",
        "\n",
        "<!-- TODO:\n",
        "- check Amirhossein Kazamnejad's blog on positional encoding -->\n",
        "\n",
        "Inspired by Umar Jamil, we use the [Harvard pytorch transformer article implementation of positional encoding formula](https://nlp.seas.harvard.edu/annotated-transformer/#positional-encoding) mentioned in the paper by using log. He mentions in his video that applying log to an exponential nullifies the effect of log but makes the calculation more numerically stable. The value of the positional encoding calculated this way will be slightly different but the model will learn. Click [here](https://youtu.be/ISNdQcPhsts?si=HNaqDgkw6CfwgO-M&t=470) to watch that particular scene from the video.\n",
        "\n",
        "Click [here](https://youtu.be/ISNdQcPhsts?si=cvEfkDJyW7LiBqkn&t=720) to see the reasoning behind using `self.register_buffer(\"pe\", pe)`. The reasoning that when we want to save some variable not as a learned parameter (like weights and biases) but we want it to be saved when we save the file of the model, the we should register it as a buffer. This way it will be saved along with the state of the model.\n",
        "\n",
        "Original formula:\n",
        "\n",
        "$$PE_{(pos, 2i)} = sin \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)$$\n",
        "\n",
        "$$PE_{(pos, 2i+1)} = cos \\left( \\frac{pos}{10000^{\\frac{2i}{d_{model}}}} \\right)$$\n",
        "\n",
        "<br></br>\n",
        "\n",
        "<!-- Modified formula by Harvard Transformer article: -->\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "816c979a",
      "metadata": {
        "id": "816c979a"
      },
      "outputs": [],
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.d_model = d_model  # in this paper, it 512\n",
        "        self.seq_len = seq_len  # maximum length of the sequence\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        # create a matrix of shape (seq_len, d_model)\n",
        "        # pe stands for positional encoding\n",
        "        pe = torch.zeros(seq_len, d_model)\n",
        "        # create a vector of shape (seq_len, 1)\n",
        "        position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "        # now, we will create the denominator of the positional encoding formulae\n",
        "        # since it is a bit long, we will break it into a few lines\n",
        "        # first, we need a vector containing multiples of 2 from 0 to d_model (here, 512)\n",
        "        # this line is because of the 2i term which is the power of 10000\n",
        "        # thus, this vector provides for the numbers we need for 2i\n",
        "        vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "        # now, we raise 10,000 to the power of 2i/d_model\n",
        "        denominator_original = torch.pow(10000, vector/d_model)\n",
        "        # this is the one used by Harvard Transformer article\n",
        "        denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
        "        # we apply sin for even dimension and cos for odd dimenion\n",
        "        # apply sin and store it in even indices of pe\n",
        "        pe[:, 0::2] = torch.sin(position * denominator_original)\n",
        "        # apply cos and store it in odd indices of pe\n",
        "        pe[:, 1::2] = torch.cos(position * denominator_original)\n",
        "        # we need to add the batch dimension so that we can apply it to\n",
        "        # batches of sentences\n",
        "        pe = pe.unsqueeze(0)  # new shape: (1, seq_len, d_model)\n",
        "        # register the pe tensor as a buffer so that it can be saved along with the\n",
        "        # state of the model\n",
        "        self.register_buffer(\"pe\", pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # we don't want to train the positional encoding, ie, we don't want to make it\n",
        "        # a learnable parameter, so we set its requires_grad to False\n",
        "        x = x + self.pe[:, :x.size(1)].requires_grad_(False)  # (batch, seq_len, d_model)\n",
        "        return self.dropout(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a3638a85",
      "metadata": {
        "id": "a3638a85"
      },
      "source": [
        "Let's see how the positional encoding works by doing it on a smaller example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "54913f49",
      "metadata": {
        "id": "54913f49",
        "outputId": "45b4fb2f-c2b1-4cdc-d16e-173149a650fe"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[ 0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,  0.0000,  1.0000,\n",
            "          0.0000,  1.0000],\n",
            "        [ 0.8415,  0.5403,  0.0264,  0.9997,  0.8573, -0.5148, -0.1383,  0.9904,\n",
            "          0.9992,  0.0402],\n",
            "        [ 0.9093, -0.4161,  0.0528,  0.9986, -0.8827, -0.4699, -0.2739,  0.9618,\n",
            "          0.0803, -0.9968],\n",
            "        [ 0.1411, -0.9900,  0.0791,  0.9969,  0.0516,  0.9987, -0.4042,  0.9147,\n",
            "         -0.9927, -0.1205],\n",
            "        [-0.7568, -0.6536,  0.1054,  0.9944,  0.8296, -0.5584, -0.5268,  0.8500,\n",
            "         -0.1600,  0.9871],\n",
            "        [-0.9589,  0.2837,  0.1316,  0.9913, -0.9058, -0.4237, -0.6393,  0.7690,\n",
            "          0.9799,  0.1993],\n",
            "        [-0.2794,  0.9602,  0.1577,  0.9875,  0.1031,  0.9947, -0.7395,  0.6732,\n",
            "          0.2392, -0.9710],\n",
            "        [ 0.6570,  0.7539,  0.1837,  0.9830,  0.7997, -0.6005, -0.8254,  0.5645,\n",
            "         -0.9606, -0.2778],\n",
            "        [ 0.9894, -0.1455,  0.2095,  0.9778, -0.9265, -0.3764, -0.8955,  0.4450,\n",
            "         -0.3160,  0.9488],\n",
            "        [ 0.4121, -0.9111,  0.2353,  0.9719,  0.1543,  0.9880, -0.9485,  0.3168,\n",
            "          0.9354,  0.3536]])\n",
            "\n",
            "\n",
            "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.8415,  0.0264,  0.8573, -0.1383,  0.9992],\n",
            "        [ 0.9093,  0.0528, -0.8827, -0.2739,  0.0803],\n",
            "        [ 0.1411,  0.0791,  0.0516, -0.4042, -0.9927],\n",
            "        [-0.7568,  0.1054,  0.8296, -0.5268, -0.1600],\n",
            "        [-0.9589,  0.1316, -0.9058, -0.6393,  0.9799],\n",
            "        [-0.2794,  0.1577,  0.1031, -0.7395,  0.2392],\n",
            "        [ 0.6570,  0.1837,  0.7997, -0.8254, -0.9606],\n",
            "        [ 0.9894,  0.2095, -0.9265, -0.8955, -0.3160],\n",
            "        [ 0.4121,  0.2353,  0.1543, -0.9485,  0.9354]])\n",
            "\n",
            "\n",
            "tensor([[ 1.0000,  1.0000,  1.0000,  1.0000,  1.0000],\n",
            "        [ 0.5403,  0.9997, -0.5148,  0.9904,  0.0402],\n",
            "        [-0.4161,  0.9986, -0.4699,  0.9618, -0.9968],\n",
            "        [-0.9900,  0.9969,  0.9987,  0.9147, -0.1205],\n",
            "        [-0.6536,  0.9944, -0.5584,  0.8500,  0.9871],\n",
            "        [ 0.2837,  0.9913, -0.4237,  0.7690,  0.1993],\n",
            "        [ 0.9602,  0.9875,  0.9947,  0.6732, -0.9710],\n",
            "        [ 0.7539,  0.9830, -0.6005,  0.5645, -0.2778],\n",
            "        [-0.1455,  0.9778, -0.3764,  0.4450,  0.9488],\n",
            "        [-0.9111,  0.9719,  0.9880,  0.3168,  0.3536]])\n"
          ]
        }
      ],
      "source": [
        "def dummyfn1():\n",
        "    seq_len = 10\n",
        "    d_model = 10\n",
        "    pe = torch.zeros(seq_len, d_model)\n",
        "    position = torch.arange(0, seq_len, dtype=torch.float32).unsqueeze(1)\n",
        "    vector = torch.arange(0, d_model, 2, dtype=torch.float32)\n",
        "    denominator_original = torch.pow(10000, vector/d_model)\n",
        "    denominator_harvard = torch.exp(vector * (-math.log(10000.0)/d_model))\n",
        "    pe[:, 0::2] = torch.sin(position * denominator_original)\n",
        "    pe[:, 1::2] = torch.cos(position * denominator_original)\n",
        "    print(pe, pe[:, 0::2], pe[:, 1::2], sep='\\n\\n\\n')\n",
        "\n",
        "dummyfn1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b5693925",
      "metadata": {
        "id": "b5693925",
        "outputId": "7635b3cf-be70-4cfd-9d6a-58935c381ac2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 2.4086,  3.1091,  1.1259, -0.0000],\n",
              "         [ 1.8999, -0.8678, -0.6868, -0.9279],\n",
              "         [ 0.1965,  1.5407, -1.5822, -0.0000],\n",
              "         [-0.0000, -1.9368, -2.2107,  0.9254]]])"
            ]
          },
          "execution_count": 90,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def dummyfn2():\n",
        "    torch.manual_seed(42)\n",
        "    seq_len = 4\n",
        "    d_model = 4\n",
        "    dropout = 0.2\n",
        "    x = torch.randn(d_model, seq_len)\n",
        "    obj = PositionalEncoding(d_model, seq_len, dropout)\n",
        "    return obj(x)\n",
        "\n",
        "dummyfn2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eeb7adc9",
      "metadata": {
        "id": "eeb7adc9"
      },
      "source": [
        "### Layer Normalization\n",
        "\n",
        "In layer normalization, we calculate the mean and variance of each data point independently from other data points. Then, we calculate new values for each data point using their own mean and their own variance.\n",
        "\n",
        "<!-- ![Screenshot%20from%202023-12-01%2000-51-48.png](attachment:Screenshot%20from%202023-12-01%2000-51-48.png) -->\n",
        "\n",
        "(Source: https://youtu.be/ISNdQcPhsts?si=_1mO7CBcvFHg15cJ.)\n",
        "\n",
        "Note: $\\text{variance} = \\text{(standard deviation)}^2$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d84276a0",
      "metadata": {
        "id": "d84276a0"
      },
      "source": [
        "We will use this formula:\n",
        "\n",
        "$$\\hat{x}_j = \\alpha \\times \\left(\\frac{x_j - \\mu_j}{\\sqrt{\\sigma_j^2 + \\epsilon}}\\right) + \\beta $$\n",
        "\n",
        ", where:\n",
        "- $\\alpha$ is the multiplicative factor\n",
        "- $\\beta$ is the additive factor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac9aac5c",
      "metadata": {
        "id": "ac9aac5c"
      },
      "outputs": [],
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "    def __init__(self, eps: float = 1e-6) -> None:\n",
        "        super(LayerNormalization, self).__init__()\n",
        "        self.eps = eps\n",
        "        # instead of simply doing self.alpha = torch.ones(1)\n",
        "        # we use nn.Parameter() so that when we call the state dict of the model\n",
        "        # we are able to see this alpha\n",
        "        # only using torch.ones(1) won't allow us to see this alpha\n",
        "        self.alpha = nn.Parameter(torch.ones(1))  # multiplied\n",
        "        self.bias = nn.Parameter(torch.zeros(1))  # added\n",
        "\n",
        "    def forward(self, x):\n",
        "        # apply mean after the batch dimension\n",
        "        # mean usually cancels the dimension to which it is applied,\n",
        "        # but we want to keep it\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        # similarly for standard deviation\n",
        "        std = x.std(dim=-1, keepdim=True)\n",
        "        return self.alpha * ((x-mean)/(std**2 + self.eps)) + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "894ecbed",
      "metadata": {
        "id": "894ecbed"
      },
      "source": [
        "### Position-Wise Feed Forward Networks\n",
        "\n",
        "See section 3.3 on page 5 of the paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d63fb799",
      "metadata": {
        "id": "d63fb799"
      },
      "outputs": [],
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    \"\"\"Implements the FFN equation.\"\"\"\n",
        "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)   # W1 and B1\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)   # W2 and B2\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        x is of the shape: (batch, seq_len, d_model)\n",
        "        linear1 is of the shape: (d_model, d_ff)\n",
        "        linear2 is of the shape: (d_ff, d_model)\n",
        "\n",
        "        On multiplying x with linear1, the shape of x becomes (batch, seq_len, d_ff)\n",
        "        On multiplying the new x with linear2, the shape of x changes back to the\n",
        "        original one, ie, (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "        x = self.relu(self.linear1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.linear2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8e65ea4",
      "metadata": {
        "id": "f8e65ea4",
        "outputId": "22241d64-7f51-4af7-dd51-23c251f2c354"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([1, 10, 10])"
            ]
          },
          "execution_count": 107,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def dummyfn1():\n",
        "    seq_len, d_model, d_ff, dropout = 10, 10, 20, 0.1\n",
        "    x = torch.randn(1, seq_len, d_model)\n",
        "    ffn = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "    return ffn(x)\n",
        "\n",
        "dummyfn1().shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84977407",
      "metadata": {
        "id": "84977407"
      },
      "source": [
        "### Multi-Head Attention\n",
        "\n",
        "Queries, Keys, and Values are all just the duplication of the input for the encoder. In other words, in the encoder block, we store the same value of input in queries, keys, and values. So, they are all the same thing. You can also think of them as just the input used 3 times."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0612ede",
      "metadata": {
        "id": "e0612ede"
      },
      "source": [
        "<!-- ![Screenshot%20from%202023-12-01%2001-36-31.png](attachment:Screenshot%20from%202023-12-01%2001-36-31.png)\n",
        "\n",
        "(Source: https://youtu.be/ISNdQcPhsts?si=_1mO7CBcvFHg15cJ.) -->"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aaa3c34",
      "metadata": {
        "id": "9aaa3c34"
      },
      "source": [
        "Pay close attention to the above figure when coding the multi-head attention class. This will help you understand what comes when and how."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cf83189a",
      "metadata": {
        "id": "cf83189a"
      },
      "source": [
        "Check [this](https://sentry.io/answers/difference-between-staticmethod-and-classmethod-function-decorators-in-python/#:~:text=We%20can%20decorate%20a%20function,object%20to%20it%2C%20as%20below.&text=This%20can%20be%20useful%20when,the%20instance%20it's%20called%20on.) article for information on `@staticmethod`. Basically, when you put `@staticmethod` on top of a method in a class, then that method does not take the `self` argument, which is the object of the class."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "969f2dac",
      "metadata": {
        "id": "969f2dac"
      },
      "source": [
        "**Scaled dot-product attention**:\n",
        "\n",
        "$$\\text{Attention(Q,K,V)} = \\text{softmax} \\left( \\frac{Q K^T}{\\sqrt{d_k}} \\right)$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "12cd22ac",
      "metadata": {
        "id": "12cd22ac"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttentionBlock(nn.Module):\n",
        "    def __init__(self, d_model: int, h: int, dropout: float) -> None:\n",
        "        \"\"\"Take in model size and number of heads.\"\"\"\n",
        "        super(MultiHeadAttention, self).__init__()\n",
        "        self.d_model = d_model  # embedding vector size\n",
        "        self.h = h   # number of heads\n",
        "        # make sure d_model is divisible by h\n",
        "        assert d_model % h == 0, \"d_model is not divisible by h\"\n",
        "        # we assume d_v always equals d_k\n",
        "        self.d_k = d_model // h  # dimension of vector seen by each head\n",
        "        self.wq = nn.Linear(d_model, d_model, bias=False)  # Wq\n",
        "        self.wk = nn.Linear(d_model, d_model, bias=False)  # Wk\n",
        "        self.wv = nn.Linear(d_model, d_model, bias=False)  # Wv\n",
        "        self.wo = nn.Linear(d_model, d_model, bias=False)  # Wo\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    @staticmethod\n",
        "    def attention(key, query, value, mask=None, dropout=None):\n",
        "        \"\"\"Compute scaled dot-product attention\"\"\"\n",
        "        d_k = query.size(-1)\n",
        "        # calculate the attention scores by applying scaled dot-product attention\n",
        "        # (batch, h, seq_len, seq_len) --> (batch, h, seq_len, d_k)\n",
        "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
        "        if mask is not None:\n",
        "            # write a very low value (indicating -infinity) to the positions\n",
        "            # where mask == 0, this will tell softmax to replace those values\n",
        "            # with zero\n",
        "            scores = scores.masked_fill(mask==0, -1e9)\n",
        "            # alternatively, we can use the inplace operation masked_fill_\n",
        "            # , where '_' after 'masked_fill' indicates inplace operation\n",
        "            # scores.masked_fill_(mask==0, -1e9)\n",
        "        # now, we convert the attention scores to probability scores by\n",
        "        # applying softmax\n",
        "        # note: all the probability scores of a particular datapoint must sum upto 1\n",
        "        prob_scores = scores.softmax(dim=-1)  # (batch, h, seq_len, seq_len)\n",
        "        if dropout is not None:\n",
        "            prob_scores = dropout(prob_scores)\n",
        "        # now, we (matrix) multiply prob_scores with value\n",
        "        # so the shape changes from (batch, h, seq_len, seq_len)\n",
        "        # to (batch, h, seq_len, d_k)\n",
        "        # we also return the prob_scores, which can be used for visualization\n",
        "        return torch.matmul(prob_scores, value), prob_scores\n",
        "\n",
        "    def forward(self, q, k, v, mask=None):\n",
        "        # multiply Wq matrix by q\n",
        "        # this matrix multiplication does not change the shape of q\n",
        "        query = self.wq(q)  # (batch, h, seq_len)\n",
        "        # similarly for key and value\n",
        "        key = self.wk(k)    # (batch, h, seq_len)\n",
        "        value = self.wv(v)  # (batch, h, seq_len)\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, h, d_k)\n",
        "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k)\n",
        "        # (batch, seq_len, h, d_k) --> (batch, h, seq_len, d_k)\n",
        "        query = query.transpose(1,2) # interchange the indices 1 and 2 with each other\n",
        "        # similarly the dimensions of key and value will also change\n",
        "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k)\n",
        "        key = key.transpose(1,2)\n",
        "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k)\n",
        "        value = value.transpose(1,2)\n",
        "        # calculate attention\n",
        "        x, self.attn_scores =  MultiHeadAttentionBlock.attention(query, key, value,\n",
        "                                                            mask, self.dropout)\n",
        "        # combine all the heads together\n",
        "        # (batch, h, seq_len, d_k) --> (batch, seq_len, h, d_k)\n",
        "        x = x.transpose(1,2)\n",
        "        # (batch, seq_len, h, d_k) --> (batch, seq_len, d_model)\n",
        "        x = x.contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
        "        # alternative code to the above line:\n",
        "        # x = x.reshape(x.shape[0], -1, self.h * self.d_k)\n",
        "        # now, multiply by Wo\n",
        "        # this matrix multiplication does not change the shape of x\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, d_model)\n",
        "        return self.wo(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b1aa966",
      "metadata": {
        "id": "5b1aa966"
      },
      "source": [
        "`.view()` is used to reshape a tensor. We can reshape a tensor using `.view()` and stored the reshaped version in another variable. We must note that `.view()` passes the reference of a tensor, ie, memory address of the tensor. So, if we make changes to one tensor, then they get reflected in the other tensor as well.\n",
        "\n",
        "<br></br>\n",
        "\n",
        "Check [this](https://stackoverflow.com/questions/48915810/what-does-contiguous-do-in-pytorch) for information on the use of `.contiguous()` in PyTorch."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d01c904c",
      "metadata": {
        "id": "d01c904c",
        "outputId": "5fc07e4b-98a9-4585-f9fb-91604617e4d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1., 1., 1.],\n",
            "        [0., 1., 1.]])\n",
            "\n",
            "tensor([[1., 1., 1., 0., 1., 1.]])\n"
          ]
        }
      ],
      "source": [
        "def dummyfn1():\n",
        "    A = torch.ones(2,3)\n",
        "    B = A.view(1,6)\n",
        "    B[0,3] = 0  # change 1 at index [0,3] in B, this will also change the\n",
        "    # the 1 in A at the corresponding index\n",
        "    print(A, B, sep='\\n\\n')\n",
        "    pass\n",
        "\n",
        "dummyfn1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6b70910",
      "metadata": {
        "id": "d6b70910",
        "outputId": "3b38b214-1a5f-4ad8-e0e9-54d5d4e6bbbe"
      },
      "outputs": [
        {
          "ename": "RuntimeError",
          "evalue": "view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead.",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_31516/3471510682.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mdummyfn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipykernel_31516/3471510682.py\u001b[0m in \u001b[0;36mdummyfn1\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mA\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead."
          ]
        }
      ],
      "source": [
        "def dummyfn1():\n",
        "    A = torch.ones(3,4)\n",
        "    A = A.transpose(-1,-2)\n",
        "    A = A.view(A.shape[1], -1)  # we get error because we didn't use contiguous\n",
        "    return A\n",
        "\n",
        "dummyfn1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67fe869e",
      "metadata": {
        "id": "67fe869e",
        "outputId": "c9761fb9-a16f-42a1-9e73-4e13040cca40"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def dummyfn1():\n",
        "    A = torch.ones(3,4)\n",
        "    A = A.transpose(-1,-2)\n",
        "    A = A.contiguous().view(A.shape[1], -1)\n",
        "    return A\n",
        "\n",
        "dummyfn1()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d73a55a2",
      "metadata": {
        "id": "d73a55a2",
        "outputId": "2a7e66c0-f725-483c-9d76-77490028b5ea"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.],\n",
              "        [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 138,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def dummyfn1():\n",
        "    A = torch.ones(3,4)\n",
        "    A = A.transpose(-1,-2)\n",
        "    # instead of using contiguous and view, we can use reshape\n",
        "    A = A.reshape(A.shape[1], -1)\n",
        "    return A\n",
        "\n",
        "dummyfn1()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2edba86a",
      "metadata": {
        "id": "2edba86a"
      },
      "source": [
        "**3 ways of doing transpose in PyTorch**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ac1f08ef",
      "metadata": {
        "id": "ac1f08ef",
        "outputId": "03e4de1d-0c54-4638-e5d9-ec73ec162ccf"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.]])"
            ]
          },
          "execution_count": 116,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ones(2,3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdd5dd57",
      "metadata": {
        "id": "cdd5dd57",
        "outputId": "187253e4-ea2c-49d3-d801-48e5e7bb9eb5"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "execution_count": 118,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.transpose(torch.ones(2,3), -1, -2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9123f85",
      "metadata": {
        "id": "b9123f85",
        "outputId": "98943c98-8263-4f03-8120-0379336c6d4e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[1., 1.],\n",
              "        [1., 1.],\n",
              "        [1., 1.]])"
            ]
          },
          "execution_count": 119,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.ones(2,3).T"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "70d8ee36",
      "metadata": {
        "id": "70d8ee36"
      },
      "source": [
        "### Residual Connection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e6b8a52a",
      "metadata": {
        "id": "e6b8a52a"
      },
      "outputs": [],
      "source": [
        "class ResidualConnection(nn.Module):\n",
        "    \"\"\"This is the 'add' part in the 'add and norm' block.\"\"\"\n",
        "    def __init__(self, features: int, dropout: float) -> None:\n",
        "        super(ResidualConnection, self).__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, sublayer):\n",
        "        \"\"\"\n",
        "        x: input\n",
        "        sublayer: different layers of the transformer architecture (eg: multi-head\n",
        "        attention, feed-forward network, etc.)\n",
        "\n",
        "        Returns the skip or residual connection.\n",
        "        \"\"\"\n",
        "        # most implementations first do normalization and then pass x to the sublayer\n",
        "        # we will also do this way\n",
        "        return x + self.dropout(sublayer(self.norm(x)))\n",
        "        # however, the paper first passes x to the sublayer and then does the norm\n",
        "        # return x + self.dropout(self.norm(sublayer(x)))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "482a9715",
      "metadata": {
        "id": "482a9715"
      },
      "source": [
        "### Encoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cd157121",
      "metadata": {
        "id": "cd157121"
      },
      "outputs": [],
      "source": [
        "class EncoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, selfattn_block: MultiHeadAttentionBlock,\n",
        "                 feedforward_block: PositionWiseFeedForward, dropout: float) -> None:\n",
        "        self.selfattn_block = selfattn_block\n",
        "        self.feedforward_block = feedforward_block\n",
        "        # store 2 residual connection layers\n",
        "        # we'l use one after self-attention layer and the other after feed-forward\n",
        "        # network as shown in figure 1 of the paper\n",
        "        self.res_con = nn.ModuleList([ResidualConnection(features, dropout)\n",
        "                                      for _ in range(2)])\n",
        "\n",
        "    def forward(self, x, src_mask):\n",
        "        # we apply the source mask because we don't want the padding word to\n",
        "        # interact with other words\n",
        "        x = self.res_con[0](x, lambda x: self.selfattn_block(x,x,x,src_mask))\n",
        "        x = self.res_con[1](x, self.feedforward_block)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9196139c",
      "metadata": {
        "id": "9196139c",
        "outputId": "fc9ef115-03ea-4ccb-a1e1-4cc6c29866d1"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "ModuleList(\n",
              "  (0): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (1): Linear(in_features=2, out_features=2, bias=True)\n",
              "  (2): Linear(in_features=2, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nn.ModuleList(nn.Linear(2,2) for _ in range(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "38763070",
      "metadata": {
        "id": "38763070"
      },
      "outputs": [],
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, features: int, layers: nn.ModuleList) -> None:\n",
        "        super(Encoder, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4655a068",
      "metadata": {
        "id": "4655a068"
      },
      "source": [
        "### Decoder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "58ab1de1",
      "metadata": {
        "id": "58ab1de1"
      },
      "outputs": [],
      "source": [
        "class DecoderBlock(nn.Module):\n",
        "    def __init__(self, features: int, selfattn_block: MultiHeadAttentionBlock,\n",
        "                 crossattn_block: MultiHeadAttentionBlock, dropout: float,\n",
        "                 feedforward_block: PositionWiseFeedForward) -> None:\n",
        "        super(DecoderBlock, self).__init__()\n",
        "        self.selfattn_block = selfattn_block\n",
        "        self.crossattn_block = crossattn_block\n",
        "        self.feedforward_block = feedforward_block\n",
        "        self.res_con = nn.ModuleList([ResidualConnection(features, dropout)\n",
        "                                      for _ in range(3)])\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        x = self.res_con[0](x, lambda x: self.selfattn_block(x, x, x, tgt_mask))\n",
        "        x = self.res_con[1](x, lambda x: self.crossattn_block(x, encoder_output,\n",
        "                                                              encoder_output, src_mask))\n",
        "        x = self.res_con[2](x, self.feedforward_block)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4669f985",
      "metadata": {
        "id": "4669f985"
      },
      "outputs": [],
      "source": [
        "class Decoder(nn.Module):\n",
        "    def __init__(self, features: int, layers: nn.ModuleList):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.layers = layers\n",
        "        self.norm = LayerNormalization(features)\n",
        "\n",
        "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
        "        for layer in self.layers:\n",
        "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
        "        return self.norm(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ce837c8",
      "metadata": {
        "id": "8ce837c8"
      },
      "outputs": [],
      "source": [
        "class ProjectionLayer(nn.Module):\n",
        "    def __init__(self, d_model, vocab_size) -> None:\n",
        "        super(ProjectionLayer, self).__init__()\n",
        "        self.proj = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # (batch, seq_len, d_model) --> (batch, seq_len, vocab_size)\n",
        "        return self.proj(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "231ffd76",
      "metadata": {
        "id": "231ffd76"
      },
      "source": [
        "### The Transformer Class (collection of all the above methods)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a3fca369",
      "metadata": {
        "id": "a3fca369"
      },
      "outputs": [],
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbedding,\n",
        "                 tgt_embed: InputEmbedding, src_pos: PositionalEncoding,\n",
        "                 tgt_pos: PositionalEncoding, proj_layer: ProjectionLayer) -> None:\n",
        "        self.encoder = encoder\n",
        "        self.decoder = decoder\n",
        "        self.src_embed = src_embed\n",
        "        self.tgt_embed = tgt_embed\n",
        "        self.src_pos = src_pos\n",
        "        self.tgt_pos = tgt_pos\n",
        "        self.proj_layer = proj_layer\n",
        "\n",
        "    def encode(self, src, src_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        src = self.src_embed(src)\n",
        "        src = self.src_pos(src)\n",
        "        return self.encoder(src, src_mask)\n",
        "\n",
        "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
        "        # (batch, seq_len, d_model)\n",
        "        tgt = self.tgt_embed(tgt)\n",
        "        tgt = self.tgt_pos(tgt)\n",
        "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
        "\n",
        "    def project(self, x):\n",
        "        # (batch, seq_len, vocab_size)\n",
        "        return self.proj_layer(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c3b84bd8",
      "metadata": {
        "id": "c3b84bd8"
      },
      "source": [
        "### Final Transformer Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5a08ded",
      "metadata": {
        "id": "c5a08ded"
      },
      "outputs": [],
      "source": [
        "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int,\n",
        "                      tgt_seq_len: int, d_model: int = 512, Nx: int = 6, h: int = 8,\n",
        "                      dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
        "    # Create the input and output embedding layers\n",
        "    src_embed = InputEmbedding(d_model, src_vocab_size)\n",
        "    tgt_embed = InputEmbedding(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the input and output positional encoding layers\n",
        "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout)\n",
        "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout)\n",
        "\n",
        "    # Create the encoder blocks\n",
        "    encoder_blocks = []\n",
        "    for _ in range(Nx):\n",
        "        encoder_selfattn_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        encoder_feedforward_block = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "        encoder_block = EncoderBlock(d_model, encoder_selfattn_block,\n",
        "                                     encoder_feedforward_block, dropout)\n",
        "        encoder_blocks.append(encoder_block)\n",
        "\n",
        "    # Create the decoder blocks\n",
        "    decoder_blocks = []\n",
        "    for _ in range(Nx):\n",
        "        decoder_selfttn_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_crossattn_block = MultiHeadAttentionBlock(d_model, h, dropout)\n",
        "        decoder_feedforward_block = PositionWiseFeedForward(d_model, d_ff, dropout)\n",
        "        decoder_block = DecoderBlock(d_model, decoder_selfttn_block,\n",
        "                                     decoder_crossattn_block, decoder_feedforward_block,\n",
        "                                     dropout)\n",
        "        decoder_blocks.append(decoder_block)\n",
        "\n",
        "    # Create the encoder and decoder\n",
        "    encoder = Encoder(d_model, nn.ModuleList(encoder_blocks))\n",
        "    decoder = Decoder(d_model, nn.ModuleList(decoder_blocks))\n",
        "\n",
        "    # Create the projection layer\n",
        "    proj_layer = ProjectionLayer(d_model, tgt_vocab_size)\n",
        "\n",
        "    # Create the transformer\n",
        "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos,\n",
        "                              tgt_pos, projection_layer)\n",
        "\n",
        "    # Initialize the parameters\n",
        "    for p in transformer.parameters():\n",
        "        if p.dim() > 1:\n",
        "            nn.init.xavier_uniform_(p)\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "054d3d80",
      "metadata": {
        "id": "054d3d80"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}